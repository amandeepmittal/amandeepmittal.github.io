{"componentChunkName":"component---src-templates-post-js","path":"/how-to-build-a-xylophone-app-with-audio-api-react-native-and-expo","webpackCompilationHash":"c3a562291cd8e443a353","result":{"data":{"site":{"siteMetadata":{"title":"Aman Mittal - Fullstack Developer","description":"Aman Mittal - Developer and Technical writer.","author":{"name":"Aman Mittal"},"keywords":["Fullstack Developer"]}},"mdx":{"frontmatter":{"title":"How to build a xylophone app with Audio API, React Native, and Expo","date":"July 22, 2019","author":"Aman Mittal","banner":null,"slug":"how-to-build-a-xylophone-app-with-audio-api-react-native-and-expo","keywords":null},"code":{"body":"function _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"slug\": \"how-to-build-a-xylophone-app-with-audio-api-react-native-and-expo\",\n  \"date\": \"2019-07-22T00:00:00.000Z\",\n  \"title\": \"How to build a xylophone app with Audio API, React Native, and Expo\",\n  \"categories\": [\"react native\"],\n  \"description\": \"---\",\n  \"published\": true,\n  \"author\": \"Aman Mittal\",\n  \"banner\": null\n};\n\nvar makeShortcode = function makeShortcode(name) {\n  return function MDXDefaultShortcode(props) {\n    console.warn(\"Component \" + name + \" was not imported, exported, or provided by MDXProvider as global scope\");\n    return mdx(\"div\", props);\n  };\n};\n\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, [\"components\"]);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"p\", null, mdx(\"img\", _extends({\n    parentName: \"p\"\n  }, {\n    \"src\": \"https://miro.medium.com/max/1920/1*Ys5lHm_e9MOF82uNJcPjFQ.jpeg\",\n    \"alt\": null\n  }))), mdx(\"p\", null, \"React Native when used with Expo as a toolchain eases out the common pain to manage ios and android applications. After saying that, I realized that there is a delight to use this ever-growing open source mobile application framework. Expo has gained a lot of credibility as a framework to provide collective solutions to build React Native applications by lowering the time and effort of the developer using it. They are continuing to enhance it from time to time and keeping up with the latest changes in React Native community. That said, Expo SDK33 is a blast.\"), mdx(\"p\", null, \"That being said, let us dive into one of the Expo's API. In this tutorial, you are going to build an application using Expo's Audio API. You are going to develop the following app (\", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"a toy xylophone app\"), \") step-by-step.\"), mdx(\"p\", null, mdx(\"img\", _extends({\n    parentName: \"p\"\n  }, {\n    \"src\": \"https://miro.medium.com/max/350/1*YE5sb3gX_ValSM48QGT1eQ.png\",\n    \"alt\": \"ss2\"\n  }))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Table of Contents\")), mdx(\"h2\", null, \"Requirements\"), mdx(\"p\", null, \"To follow this tutorial, please make sure you have the following installed on your local development environment and have access to the services mentioned below:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", _extends({\n    parentName: \"li\"\n  }, {\n    \"href\": \"https://nodejs.org/en/\"\n  }), \"Nodejs\"), \" (>=\", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"8.x.x\"), \") with npm/yarn installed.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", _extends({\n    parentName: \"li\"\n  }, {\n    \"href\": \"https://docs.expo.io/versions/latest/workflow/expo-cli/\"\n  }), mdx(\"inlineCode\", {\n    parentName: \"a\"\n  }, \"expo-cli\")), \" (>= \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"2.19.4)\"), \", previously known as create-react-native-app.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", _extends({\n    parentName: \"li\"\n  }, {\n    \"href\": \"https://facebook.github.io/watchman/\"\n  }), mdx(\"inlineCode\", {\n    parentName: \"a\"\n  }, \"watchman\")), \" is the file change watcher for React Native projects.\")), mdx(\"h2\", null, \"Getting Started\"), mdx(\"p\", null, \"To create a new Expo project, the only requirement is to have \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"expo-cli\"), \" installed. Then, execute the following command to create a new project directory.\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-shell\"\n  }), \"expo init rn-xylophone-app\\n\\n# navigate inside the app folder\\ncd rn-xylophone-app\\n\\n# install the following dependency\\nyarn add expo-av\\n\")), mdx(\"p\", null, \"Once the project directory is generated, navigate inside it as shown in the above command. Then install the required dependency to add the functionality of playing an Audio file inside the React Native app. The dependency \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"expo-av\"), \" will help you use Audio API and its promise based asynchronous methods to play the audio files. You are going to implement this functionality later.\"), mdx(\"p\", null, \"The last step needed is to have some sound files saved in your \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"assets\"), \" folder. You can, of course, use your audio files but if you want to use the same audio files used in this tutorial, you can download them at the link given below.\"), mdx(\"p\", null, \"[add assets folder download link]\"), mdx(\"p\", null, \"You might have got the idea of what the user interface is going to look while having a glimpse at the demo in the previous section. For each button, you are going to need a different color. Hence, create a new file called \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"contants/Colors.js\"), \" and add the following code.\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-js\"\n  }), \"export const NoteOne = 'red'\\nexport const NoteTwo = 'orange'\\nexport const NoteThree = 'yellow'\\nexport const NoteFour = 'green'\\nexport const NoteFive = '#00FFFF    '\\nexport const NoteSix = '#000080'\\nexport const NoteSeven = '#B266FF'\\n\")), mdx(\"p\", null, \"Require this file and all the Color codes inside \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"App.js\"), \" file after other imports.\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-js\"\n  }), \"// ...after other imports\\n\\nimport {\\n  NoteOne,\\n  NoteTwo,\\n  NoteThree,\\n  NoteFour,\\n  NoteFive,\\n  NoteSix,\\n  NoteSeven\\n} from './constants/Colors'\\n\")), mdx(\"p\", null, \"The color names are specified to mark each audio file which is named and numbered similarly. To import all the sounds file needed to build the application from the \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"assets\"), \" folder. Add the below object before the \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"App\"), \" component as shown.\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-js\"\n  }), \"const xyloSounds = {\\n  one: require('./assets/note1.wav'),\\n  two: require('./assets/note2.wav'),\\n  three: require('./assets/note3.wav'),\\n  four: require('./assets/note4.wav'),\\n  five: require('./assets/note5.wav'),\\n  six: require('./assets/note6.wav'),\\n  seven: require('./assets/note7.wav')\\n}\\n\")), mdx(\"p\", null, \"The above object \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"xyloSounds\"), \" consist of the path to each sound file. This will be helpful when you are writing business logic to play these audio files and have to detect which audio file to play for the specific note.\"), mdx(\"h2\", null, \"Building the first UI button\"), mdx(\"p\", null, \"In this section, you are going to create a button using \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"TouchableOpacity\"), \" that is going to play the sound for the note when pressed. To start, make sure in the file \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"App.js\"), \" you have imported the following APIs from the react-native core.\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-js\"\n  }), \"import { StyleSheet, Text, View, TouchableOpacity } from 'react-native'\\n\")), mdx(\"p\", null, \"Then, you have to modify the contents of the \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"render\"), \" function from the default, boilerplate text that any Expo application comes with. This is going to be done by creating a \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"View\"), \" container for each button, which will have a fixed height and margin of value \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"5\"), \" to add some spacing between the buttons.\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-js\"\n  }), \"<View style={styles.container}>\\n  <View style={styles.buttonContainer}>\\n    <TouchableOpacity\\n      style={[styles.button, { backgroundColor: NoteOne }]}\\n      onPress={() => this.handlePlaySound('one')}\\n    >\\n      <Text style={styles.buttonText}>Note 1</Text>\\n    </TouchableOpacity>\\n  </View>\\n</View>\\n\")), mdx(\"p\", null, \"Notice that each button will have its background color specified in the file \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"constants/Colors.js\"), \". This is done by inline styling method. To combine multiple styles in React Native, you can use an array notation like above. The button has one \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"onPress\"), \" method that is going to be responsible for playing the correct sound associated with the note. You will be creating the method \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"handlePlaySound\"), \" in the next section. However, do note that the value \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"one\"), \" being passed to this method is coming from the path you specified earlier for each audio file. Lastly, the button is going to have a text to display the correct audio file number.\"), mdx(\"p\", null, \"The above snippet is followed by the styles that are created using \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"StyleSheet.create()\"), \" method.\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-js\"\n  }), \"const styles = StyleSheet.create({\\n  container: {\\n    flex: 1,\\n    backgroundColor: '#fff',\\n    marginTop: 50\\n  },\\n  buttonContainer: {\\n    height: 40,\\n    margin: 5\\n  },\\n  button: {\\n    flex: 1,\\n    alignItems: 'center',\\n    justifyContent: 'center'\\n  },\\n  buttonText: {\\n    color: '#fff',\\n    fontSize: 18\\n  }\\n})\\n\")), mdx(\"p\", null, \"To see the current state of the application in action, go back to the terminal window and run the command \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"yarn start\"), \" or \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"expo start\"), \" if you do not have yarn installed. In the simulator screen, you are going to be welcomed, as shown in the below image.\"), mdx(\"p\", null, mdx(\"img\", _extends({\n    parentName: \"p\"\n  }, {\n    \"src\": \"https://miro.medium.com/max/350/1*jzwOoqsEJkTUSMMEKCwOPg.png\",\n    \"alt\": \"ss1\"\n  }))), mdx(\"h2\", null, \"Adding the Audio functionality\"), mdx(\"p\", null, \"To play a sound in an Expo application, you are required to first the API for the \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"Audio\"), \" from \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"expo-av\"), \". So at the top of the \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"App.js\"), \" file and after other imports, you can add the following line.\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-js\"\n  }), \"import { Audio } from 'expo-av'\\n\")), mdx(\"p\", null, \"Next, you have to add the method \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"handlePlaySound\"), \" inside the \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"App\"), \" function and before the \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"render()\"), \" method. Inside this function, create a new sound object. Whenever you are required to play sound using \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"expo-av\"), \" library, you have to create a new object. This object is going to represent the instance of the class \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"Audio.sound\"), \".\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-js\"\n  }), \"handlePlaySound = async note => {\\n  const soundObject = new Audio.Sound()\\n\\n  try {\\n    let source = require('./assets/note1.wav')\\n    await soundObject.loadAsync(source)\\n    await soundObject\\n      .playAsync()\\n      .then(async playbackStatus => {\\n        setTimeout(() => {\\n          soundObject.unloadAsync()\\n        }, playbackStatus.playableDurationMillis)\\n      })\\n      .catch(error => {\\n        console.log(error)\\n      })\\n  } catch (error) {\\n    console.log(error)\\n  }\\n}\\n\")), mdx(\"p\", null, \"In the above snippet, you can notice that the method \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"handlePlaySound\"), \" is going to accept one parameter. This parameter is going to be the note's number, hence the name of the parameter being passed in the above snippet is called \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"note\"), \". Inside that, the first line creates the instance of the class \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"Audio.Sound()\"), \".\"), mdx(\"p\", null, \"Since JavaScript syntax of \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"async/await\"), \" is being used, it is better to create a \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"try/catch\"), \" block such that this Expo app does not give us any error when running the application. Inside this block, the first method \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"loadAsync\"), \" is used to create and load the sound from the source. Hence, the variable \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"source\"), \" defined explicitly is passed and contains the path of the first audio file from the \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"assets\"), \" folder.\"), mdx(\"p\", null, \"To play the sound, \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"playAsync()\"), \" method is used. This method is, however, further extends using a promise that takes one object called \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"playbackStatus\"), \" object. This object uses \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"playableDurationMillis\"), \" to identify the position until the audio file should run from the memory. Once the audio file is played, the \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"soundObject\"), \" calls the method \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"unloadAsync()\"), \" which unloads the media file from memory. This allows the media file to be played again and again. The \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"setTimeout\"), \" function's value depends on the duration of the media file being played from memory.\"), mdx(\"p\", null, \"Go back to the simulator or the device the current app is running and try to press the first button. You will hear the sound of the first note.\"), mdx(\"h2\", null, \"Finishing the App\"), mdx(\"p\", null, \"To complete building the application, you have to read the path of each file from the object \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"xyloSounds.\"), \" Edit the the value of \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"source\"), \" inside the method \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"handlePlaySound()\"), \".\\nAlso, add the button for each note and do not forget to pass the correct source value inside the \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"onPress()\"), \" method. For your reference, here is the complete code of the file \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"App.js\"), \".\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-js\"\n  }), \"import React from 'react'\\nimport { StyleSheet, Text, View, TouchableOpacity } from 'react-native'\\nimport { Audio } from 'expo-av'\\n\\nimport {\\n  NoteOne,\\n  NoteTwo,\\n  NoteThree,\\n  NoteFour,\\n  NoteFive,\\n  NoteSix,\\n  NoteSeven\\n} from './constants/Colors'\\n\\nconst xyloSounds = {\\n  one: require('./assets/note1.wav'),\\n  two: require('./assets/note2.wav'),\\n  three: require('./assets/note3.wav'),\\n  four: require('./assets/note4.wav'),\\n  five: require('./assets/note5.wav'),\\n  six: require('./assets/note6.wav'),\\n  seven: require('./assets/note7.wav')\\n}\\n\\nexport default function App() {\\n  handlePlaySound = async note => {\\n    const soundObject = new Audio.Sound()\\n\\n    try {\\n      let source = xyloSounds[note]\\n      // let source = require('./assets/note1.wav')\\n      await soundObject.loadAsync(source)\\n      await soundObject\\n        .playAsync()\\n        .then(async playbackStatus => {\\n          setTimeout(() => {\\n            soundObject.unloadAsync()\\n          }, playbackStatus.playableDurationMillis)\\n        })\\n        .catch(error => {\\n          console.log(error)\\n        })\\n    } catch (error) {\\n      console.log(error)\\n    }\\n  }\\n\\n  return (\\n    <View style={styles.container}>\\n      <View style={styles.buttonContainer}>\\n        <TouchableOpacity\\n          style={[styles.button, { backgroundColor: NoteOne }]}\\n          onPress={() => this.handlePlaySound('one')}\\n        >\\n          <Text style={styles.buttonText}>Note 1</Text>\\n        </TouchableOpacity>\\n      </View>\\n      <View style={styles.buttonContainer}>\\n        <TouchableOpacity\\n          style={[styles.button, { backgroundColor: NoteTwo }]}\\n          onPress={() => this.handlePlaySound('two')}\\n        >\\n          <Text style={styles.buttonText}>Note 2</Text>\\n        </TouchableOpacity>\\n      </View>\\n      <View style={styles.buttonContainer}>\\n        <TouchableOpacity\\n          style={[styles.button, { backgroundColor: NoteThree }]}\\n          onPress={() => this.handlePlaySound('three')}\\n        >\\n          <Text style={styles.buttonText}>Note 3</Text>\\n        </TouchableOpacity>\\n      </View>\\n      <View style={styles.buttonContainer}>\\n        <TouchableOpacity\\n          style={[styles.button, { backgroundColor: NoteFour }]}\\n          onPress={() => this.handlePlaySound('four')}\\n        >\\n          <Text style={styles.buttonText}>Note 4</Text>\\n        </TouchableOpacity>\\n      </View>\\n      <View style={styles.buttonContainer}>\\n        <TouchableOpacity\\n          style={[styles.button, { backgroundColor: NoteFive }]}\\n          onPress={() => this.handlePlaySound('five')}\\n        >\\n          <Text style={styles.buttonText}>Note 5</Text>\\n        </TouchableOpacity>\\n      </View>\\n      <View style={styles.buttonContainer}>\\n        <TouchableOpacity\\n          style={[styles.button, { backgroundColor: NoteSix }]}\\n          onPress={() => this.handlePlaySound('six')}\\n        >\\n          <Text style={styles.buttonText}>Note 6</Text>\\n        </TouchableOpacity>\\n      </View>\\n      <View style={styles.buttonContainer}>\\n        <TouchableOpacity\\n          style={[styles.button, { backgroundColor: NoteSeven }]}\\n          onPress={() => this.handlePlaySound('seven')}\\n        >\\n          <Text style={styles.buttonText}>Note 7</Text>\\n        </TouchableOpacity>\\n      </View>\\n    </View>\\n  )\\n}\\n\\nconst styles = StyleSheet.create({\\n  container: {\\n    flex: 1,\\n    backgroundColor: '#fff',\\n    marginTop: 50\\n  },\\n  buttonContainer: {\\n    height: 40,\\n    margin: 5\\n  },\\n  button: {\\n    flex: 1,\\n    alignItems: 'center',\\n    justifyContent: 'center'\\n  },\\n  buttonText: {\\n    color: '#fff',\\n    fontSize: 18\\n  }\\n})\\n\")), mdx(\"p\", null, \"Now run the application in the simulator, and you will get the following screen.\"), mdx(\"p\", null, mdx(\"img\", _extends({\n    parentName: \"p\"\n  }, {\n    \"src\": \"https://miro.medium.com/max/350/1*YE5sb3gX_ValSM48QGT1eQ.png\",\n    \"alt\": \"ss2\"\n  }))), mdx(\"h2\", null, \"Conlusion\"), mdx(\"p\", null, \"You have reached the end of this tutorial. I hope you have learned how to integrate the \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"expo-av\"), \" library to use \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"Audio\"), \" class to create functionality in your cross-platform applications and play audio media files. Important things to notice in this demo application is how to use available methods like \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"loadAsync()\"), \", \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"unloadAsync()\"), \" and leverage the duration of the playing media using the object \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"playplaybackStatus\"), \".\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://heartbeat.fritz.ai/how-to-build-a-xylophone-app-with-audio-api-react-native-and-expo-7d6754a0603c\"\n  }), \"Originally published at Heartbeat\"))));\n}\n;\nMDXContent.isMDXComponent = true;"}}},"pageContext":{"isCreatedByStatefulCreatePages":false,"id":"9aa705db-4d05-51b2-ad7f-b4c8dbaa94a0","prev":{"id":"93f827f8-5179-5932-aa89-d9b2f01fd712","parent":{"name":"index","sourceInstanceName":"blog"},"excerpt":"Google Spreadsheets and Nodejs might sound a strange combination, but the server side platform can be a great way to utilize the Google Drive API. In this tutorial, you are going to learn how to build a connect the two leveraging a Crowdbotics app…","fields":{"title":"Visualize Google Sheets Data in a NodeJS App","slug":"google-sheets-nodejs","date":"2019-07-22T00:00:00.000Z"},"code":{"scope":""}},"next":{"id":"392064ff-b891-5862-94bc-7d8cd8b69dd7","parent":{"name":"index","sourceInstanceName":"blog"},"excerpt":"Chatbots are the hottest things in the modern digital world. Every day, organizations and individuals are powering their digital products such as websites or messenger apps to provide conversational experiences for their users. Each conversational…","fields":{"title":"How to Create a Chatbot with Dialogflow, NodeJS, and Webhooks","slug":"build-chatbot-dialogflow-nodejs-webhooks","date":"2019-07-19T00:00:00.000Z"},"code":{"scope":""}}}}}